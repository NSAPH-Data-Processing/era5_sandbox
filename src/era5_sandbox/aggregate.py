# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notes/02_aggregate.ipynb.

# %% auto 0
__all__ = ['resample_netcdf', 'RasterFile', 'netcdf_to_tiff', 'polygon_to_raster_cells', 'aggregate_to_healthsheds',
           'aggregate_data', 'main']

# %% ../../notes/02_aggregate.ipynb 4
import tempfile
import rasterio
import hydra
import argparse
import os

import pandas as pd
import geopandas as gpd
import numpy as np
import xarray as xr
import matplotlib.pyplot as plt

from dataclasses import dataclass, field
from typing import Optional, Tuple
from pyprojroot import here
from hydra import initialize, compose
from omegaconf import OmegaConf, DictConfig
from tqdm import tqdm
from math import ceil, floor
from rasterstats.io import Raster
from rasterstats.utils import boxify_points, rasterize_geom

try: from era5_sandbox.core import GoogleDriver, _get_callable, describe, ClimateDataFileHandler, kelvin_to_celsius
except: from core import GoogleDriver, _get_callable, describe, ClimateDataFileHandler, kelvin_to_celsius

# %% ../../notes/02_aggregate.ipynb 8
def resample_netcdf(
        fpath: str, # Path to the netCDF file.
        resample: str = "1D", # Resampling frequency (e.g., '1H', '1D')
        agg_func: callable = np.mean, # Aggregation function (e.g., np.mean, np.sum).
        time_dim: str = "valid_time", # Name of the time dimension in the dataset.
        **xr_open_kwargs  # keywords for python's xarray module
    ) -> xr.Dataset:   
    """
    Resample a netCDF file to a specified frequency and aggregation method.
    
    Args:
        fpath (str): Path to the netCDF file.
        resample (str): Resampling frequency (e.g., '1H', '1D').
        agg_func (callable): Aggregation function (e.g., np.mean, np.sum).
    
    Returns:
        xarray.Dataset: Resampled dataset.
    """

    ds = xr.open_dataset(fpath, **xr_open_kwargs)

    if callable(agg_func):
        # Use xarray's reduce method with the callable
        return ds.resample({time_dim: resample}).reduce(agg_func)
    else:
        raise TypeError("agg_func must be a callable function like np.mean, np.max, etc.")

# %% ../../notes/02_aggregate.ipynb 12
@dataclass
class RasterFile:
    path: str
    band: int # note that this is 1-indexed
    data: Optional[np.ndarray] = field(default=None, init=False)
    transform: Optional[rasterio.Affine] = field(default=None, init=False)
    crs: Optional[str] = field(default=None, init=False)
    nodata: Optional[float] = field(default=None, init=False)
    bounds: Optional[Tuple[float, float, float, float]] = field(default=None, init=False)

    def load(self):
        """Load raster data and basic metadata."""
        with rasterio.open(self.path) as src:
            self.data = src.read(self.band)  # each day gets one rasterfile
            self.transform = src.transform
            self.crs = src.crs
            self.nodata = src.nodata
            self.bounds = src.bounds
        return self

    def shape(self) -> Optional[Tuple[int, int]]:
        """Return the shape of the raster data."""
        return self.data.shape if self.data is not None else None

    def __str__(self):
        return f"RasterFile(path='{self.path}', shape={self.shape()}, crs='{self.crs}')"

# %% ../../notes/02_aggregate.ipynb 14
def netcdf_to_tiff(
    ds: xr.Dataset, # The aggregated xarray dataset to convert.    
    band: int,      # The day to rasterise; 1 indexed just like human english
    variable: str, # The variable name to convert.
    crs: str = "EPSG:4326", # Coordinate reference system (default is WGS84).    
    ):

    """
    Convert a netCDF file to a GeoTIFF file.
    
    Args:
        fpath (str): Path to the netCDF file.
        output_path (str): Path to save the output GeoTIFF file.
        variable_name (str): Name of the variable to convert.
        time_index (int): Index of the time dimension to extract.
    """

    with tempfile.TemporaryDirectory() as tmpdirname:

        # Select the variable and time index
        variable = ds[variable]
        ds_ = variable.rio.set_spatial_dims(x_dim="longitude", y_dim="latitude")
        ds_ = ds_.rio.write_crs(crs)
        # Save as GeoTIFF
        ds_.rio.to_raster(f"{tmpdirname}/output.tif")
        # Load the raster file
        raster_file = RasterFile(path=f"{tmpdirname}/output.tif", band=band).load()

    return raster_file

# %% ../../notes/02_aggregate.ipynb 19
def polygon_to_raster_cells(
    vectors,
    raster,
    nodata=None,
    affine=None,
    all_touched=False,
    verbose=False,
    **kwargs,
):
    """Returns an index map for each vector geometry to indices in the raster source.

    Parameters
    ----------
    vectors: list of geometries

    raster: ndarray

    nodata: float

    affine: Affine instance

    all_touched: bool, optional
        Whether to include every raster cell touched by a geometry, or only
        those having a center point within the polygon.
        defaults to `False`

    Returns
    -------
    dict
        A dictionary mapping vector the ids of geometries to locations (indices) in the raster source.
    """

    cell_map = []

    with Raster(raster, affine, nodata) as rast:
        # used later to crop raster and find start row and col
        min_lon, dlon = affine.c, affine.a
        max_lat, dlat = affine.f, -affine.e
        H, W = rast.shape

        for geom in tqdm(vectors, disable=(not verbose)):
            if "Point" in geom.geom_type:
                geom = boxify_points(geom, rast)

            # find geometry bounds to crop raster
            # the raster and geometry must be in the same lon/lat coordinate system
            start_row = max(0, min(H - 1, floor((max_lat - geom.bounds[3]) / dlat)))
            start_col = min(W - 1, max(0, floor((geom.bounds[0] - min_lon) / dlon)))
            end_col = max(0, min(W - 1, ceil((geom.bounds[2] - min_lon) / dlon)))
            end_row = min(H - 1, max(0, ceil((max_lat - geom.bounds[1]) / dlat)))
            geom_bounds = (
                min_lon + dlon * start_col,  # left
                max_lat - dlat * end_row - 1e-12,  # bottom
                min_lon + dlon * end_col + 1e-12,  # right
                max_lat - dlat * start_row,  # top
            )

            # crop raster to area of interest and rasterize
            fsrc = rast.read(bounds=geom_bounds)
            rv_array = rasterize_geom(geom, like=fsrc, all_touched=all_touched)
            indices = np.nonzero(rv_array)

            if len(indices[0]) > 0:
                indices = (indices[0] + start_row, indices[1] + start_col)
                assert 0 <= indices[0].min() < rast.shape[0]
                assert 0 <= indices[1].min() < rast.shape[1]
            else:
                pass  # stop here for debug

            cell_map.append(indices)

        return cell_map

# %% ../../notes/02_aggregate.ipynb 26
def aggregate_to_healthsheds(
    res_poly2cell: list, # the result of polygon_to_raster_cells    
    raster: RasterFile, # the raster data
    shapes: gpd.GeoDataFrame, # the shapes of the health sheds
    names_column: str = "fs_uid", # the unique identifier column name of the health sheds
    aggregation_func: callable = np.nanmean, # the aggregation function
    aggregation_name: str = "mean" # the name of the aggregation function
    ) -> gpd.GeoDataFrame:
    """
    Aggregate the raster data to the health sheds.
    """

    stats = []

    for indices in res_poly2cell:
        if len(indices[0]) == 0:
            # no cells found for this polygon
            stats.append(np.nan)
        else:
            cells = raster.data[indices]
            if sum(~np.isnan(cells)) == 0:
                # no valid cells found for this polygon
                stats.append(np.nan)
                continue
            else:
                # compute MEAN of valid cells
                # but this stat can be ANYTHING
                stats.append(aggregation_func(cells))

    # clean up the result into a dataframe
    stats = pd.Series(stats)
    shapes[aggregation_name] = stats
    df = pd.DataFrame(
            {"healthshed": shapes[names_column], aggregation_name: stats}
        )
    gdf = gpd.GeoDataFrame(df, geometry=shapes.geometry.values, crs=shapes.crs)
    return gdf


# %% ../../notes/02_aggregate.ipynb 36
def aggregate_data(
        cfg: DictConfig,
        input_file: str,
        output_file: str,
        exposure_variable: str
    ) -> None:
    '''
    Aggregate raster data day-by-day and store all days and statistics as separate columns in a single Parquet file.
    '''

    if cfg.development_mode:
        describe(cfg)
        return None

    geography = cfg['query'].geography
    year = cfg['query']['year']
    month = cfg['query']['month']
    daily_aggs = cfg['aggregation']['aggregation'][exposure_variable]['hourly_to_daily']
    healthshed_aggs = cfg['aggregation']['aggregation'][exposure_variable]['daily_to_healthshed']

    # Load healthsheds
    driver = GoogleDriver(json_key_path=here() / cfg.GOOGLE_DRIVE_AUTH_JSON.path)
    drive = driver.get_drive()
    healthsheds = driver.read_healthsheds(cfg.geographies[geography].healthsheds)
    
    # Initialize output DataFrame
    result_df = healthsheds[[cfg.geographies[geography].unique_id, "geometry"]].copy()

    for daily_agg in daily_aggs:
        print(f"Processing daily aggregation: {daily_agg['name']}...")
    
        daily_agg_func = _get_callable(daily_agg['function'])

        with ClimateDataFileHandler(input_file) as handler:
            if exposure_variable in ["t2m", "d2m", "swvl1"]:
                ds_path = handler.get_dataset("instant")
            else:
                ds_path = handler.get_dataset("accum")
            resampled_nc_file = resample_netcdf(ds_path, agg_func=daily_agg_func)
        
        for healthshed_agg in healthshed_aggs:
            print(f"Aggregating to healthshed by: {healthshed_agg['name']}...")

            # Get the number of days in the dataset
            days = len(resampled_nc_file.valid_time.values)

            # Get the aggregation function for healthshed
            healthshed_agg_func = _get_callable(healthshed_agg['function'])
            days = len(resampled_nc_file.valid_time.values)

            for day in range(1, days + 1):
                print(f"Processing day {day}...")
                
                day_col = f"day_{day:02d}_daily_{daily_agg['name']}"
                resampled_tiff = netcdf_to_tiff(
                    ds=resampled_nc_file,
                    band=day,
                    variable=exposure_variable,
                    crs="EPSG:4326"
                )

                result_poly2cell = polygon_to_raster_cells(
                    vectors=healthsheds.geometry.values,
                    raster=resampled_tiff.data,
                    nodata=resampled_tiff.nodata,
                    affine=resampled_tiff.transform,
                    all_touched=True,
                    verbose=True
                )

                res = aggregate_to_healthsheds(
                    res_poly2cell=result_poly2cell,
                    raster=resampled_tiff,
                    shapes=healthsheds,
                    names_column=cfg.geographies[geography].unique_id,
                    aggregation_func=healthshed_agg_func,
                    aggregation_name=exposure_variable
                )

                result_df[day_col] = res[exposure_variable]

    print(f"Saving final monthly parquet file: {output_file}")
    result_df.to_parquet(output_file, compression="snappy")
    # return(result_df)

# %% ../../notes/02_aggregate.ipynb 41
@hydra.main(version_base=None, config_path="../../conf", config_name="config")
def main(cfg: DictConfig) -> None:
    # Parse command-line arguments
    input_file = str(snakemake.input[0])  # First input file
    output_file = str(snakemake.output[0])
    geography = str(snakemake.params.geography)
    aggregation_variable = str(snakemake.params.variable)

    variables_dict = {
        "2m_temperature": "t2m",
        "2m_dewpoint_temperature": "d2m",
        "volumetric_soil_water_layer_1": "swvl1",
        "total_precipitation": "tp"
    }

    cfg['query']['geography'] = geography
    
    aggregate_data(cfg, input_file=input_file, output_file=output_file, exposure_variable=variables_dict[aggregation_variable])

# %% ../../notes/02_aggregate.ipynb 42
#| eval: false
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False

if __name__ == "__main__" and not IN_NOTEBOOK:
    main()
