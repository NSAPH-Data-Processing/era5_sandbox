# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notes/02_aggregate.ipynb.

# %% auto 0
__all__ = ['resample_netcdf', 'RasterFile', 'netcdf_to_tiff', 'polygon_to_raster_cells', 'aggregate_to_healthsheds',
           'aggregate_data', 'main']

# %% ../../notes/02_aggregate.ipynb 4
import tempfile
import rasterio
import hydra
import argparse

import pandas as pd
import geopandas as gpd
import numpy as np
import xarray as xr
import matplotlib.pyplot as plt

from dataclasses import dataclass, field
from typing import Optional, Tuple
from pyprojroot import here
from hydra import initialize, compose
from omegaconf import OmegaConf, DictConfig
from tqdm import tqdm
from math import ceil, floor
from rasterstats.io import Raster
from rasterstats.utils import boxify_points, rasterize_geom

try: from era5_sandbox.core import GoogleDriver, _get_callable, describe
except: from core import GoogleDriver, _get_callable, describe

# %% ../../notes/02_aggregate.ipynb 8
def resample_netcdf(
        fpath: str, # Path to the netCDF file.
        resample: str = "1D", # Resampling frequency (e.g., '1H', '1D')
        agg_func: callable = np.mean, # Aggregation function (e.g., np.mean, np.sum).
        time_dim: str = "valid_time" # Name of the time dimension in the dataset.
    ) -> xr.Dataset:   
    """
    Resample a netCDF file to a specified frequency and aggregation method.
    
    Args:
        fpath (str): Path to the netCDF file.
        resample (str): Resampling frequency (e.g., '1H', '1D').
        agg_func (callable): Aggregation function (e.g., np.mean, np.sum).
    
    Returns:
        xarray.Dataset: Resampled dataset.
    """

    ds = xr.open_dataset(fpath)

    if callable(agg_func):
        # Use xarray's reduce method with the callable
        return ds.resample({time_dim: resample}).reduce(agg_func)
    else:
        raise TypeError("agg_func must be a callable function like np.mean, np.max, etc.")

# %% ../../notes/02_aggregate.ipynb 11
@dataclass
class RasterFile:
    path: str
    data: Optional[np.ndarray] = field(default=None, init=False)
    transform: Optional[rasterio.Affine] = field(default=None, init=False)
    crs: Optional[str] = field(default=None, init=False)
    nodata: Optional[float] = field(default=None, init=False)
    bounds: Optional[Tuple[float, float, float, float]] = field(default=None, init=False)

    def load(self):
        """Load raster data and basic metadata."""
        with rasterio.open(self.path) as src:
            self.data = src.read(1)  # first band
            self.transform = src.transform
            self.crs = src.crs
            self.nodata = src.nodata
            self.bounds = src.bounds
        return self

    def shape(self) -> Optional[Tuple[int, int]]:
        """Return the shape of the raster data."""
        return self.data.shape if self.data is not None else None

    def __str__(self):
        return f"RasterFile(path='{self.path}', shape={self.shape()}, crs='{self.crs}')"

# %% ../../notes/02_aggregate.ipynb 13
def netcdf_to_tiff(
    ds: xr.Dataset, # The aggregated xarray dataset to convert.    
    variable: str, # The variable name to convert.
    crs: str = "EPSG:4326", # Coordinate reference system (default is WGS84).    
    ):

    """
    Convert a netCDF file to a GeoTIFF file.
    
    Args:
        fpath (str): Path to the netCDF file.
        output_path (str): Path to save the output GeoTIFF file.
        variable_name (str): Name of the variable to convert.
        time_index (int): Index of the time dimension to extract.
    """

    with tempfile.TemporaryDirectory() as tmpdirname:

        # Select the variable and time index
        variable = ds[variable]
        ds_ = variable.rio.set_spatial_dims(x_dim="longitude", y_dim="latitude")
        ds_ = ds_.rio.write_crs(crs)
        # Save as GeoTIFF
        variable.rio.to_raster(f"{tmpdirname}/output.tif")
        # Load the raster file
        raster_file = RasterFile(f"{tmpdirname}/output.tif").load()

    return raster_file

# %% ../../notes/02_aggregate.ipynb 18
def polygon_to_raster_cells(
    vectors,
    raster,
    band=1,
    nodata=None,
    affine=None,
    all_touched=False,
    verbose=False,
    **kwargs,
):
    """Returns an index map for each vector geometry to indices in the raster source.

    Parameters
    ----------
    vectors: list of geometries

    raster: ndarray

    nodata: float

    affine: Affine instance

    all_touched: bool, optional
        Whether to include every raster cell touched by a geometry, or only
        those having a center point within the polygon.
        defaults to `False`

    Returns
    -------
    dict
        A dictionary mapping vector the ids of geometries to locations (indices) in the raster source.
    """

    cell_map = []

    with Raster(raster, affine, nodata, band) as rast:
        # used later to crop raster and find start row and col
        min_lon, dlon = affine.c, affine.a
        max_lat, dlat = affine.f, -affine.e
        H, W = rast.shape

        for geom in tqdm(vectors, disable=(not verbose)):
            if "Point" in geom.geom_type:
                geom = boxify_points(geom, rast)

            # find geometry bounds to crop raster
            # the raster and geometry must be in the same lon/lat coordinate system
            start_row = max(0, min(H - 1, floor((max_lat - geom.bounds[3]) / dlat)))
            start_col = min(W - 1, max(0, floor((geom.bounds[0] - min_lon) / dlon)))
            end_col = max(0, min(W - 1, ceil((geom.bounds[2] - min_lon) / dlon)))
            end_row = min(H - 1, max(0, ceil((max_lat - geom.bounds[1]) / dlat)))
            geom_bounds = (
                min_lon + dlon * start_col,  # left
                max_lat - dlat * end_row - 1e-12,  # bottom
                min_lon + dlon * end_col + 1e-12,  # right
                max_lat - dlat * start_row,  # top
            )

            # crop raster to area of interest and rasterize
            fsrc = rast.read(bounds=geom_bounds)
            rv_array = rasterize_geom(geom, like=fsrc, all_touched=all_touched)
            indices = np.nonzero(rv_array)

            if len(indices[0]) > 0:
                indices = (indices[0] + start_row, indices[1] + start_col)
                assert 0 <= indices[0].min() < rast.shape[0]
                assert 0 <= indices[1].min() < rast.shape[1]
            else:
                pass  # stop here for debug

            cell_map.append(indices)

        return cell_map

# %% ../../notes/02_aggregate.ipynb 25
def aggregate_to_healthsheds(
    res_poly2cell: list, # the result of polygon_to_raster_cells    
    raster: RasterFile, # the raster data
    shapes: gpd.GeoDataFrame, # the shapes of the health sheds
    names_column: str = "fs_uid", # the unique identifier column name of the health sheds
    aggregation_func: callable = np.nanmean, # the aggregation function
    aggregation_name: str = "mean" # the name of the aggregation function
    ) -> gpd.GeoDataFrame:
    """
    Aggregate the raster data to the health sheds.
    """

    stats = []

    for indices in res_poly2cell:
        if len(indices[0]) == 0:
            # no cells found for this polygon
            stats.append(np.nan)
        else:
            cells = raster.data[indices]
            if sum(~np.isnan(cells)) == 0:
                # no valid cells found for this polygon
                stats.append(np.nan)
                continue
            else:
                # compute MEAN of valid cells
                # but this stat can be ANYTHING
                stats.append(aggregation_func(cells))

    # clean up the result into a dataframe
    stats = pd.Series(stats)
    shapes[aggregation_name] = stats
    df = pd.DataFrame(
            {"healthshed": shapes[names_column], aggregation_name: stats}
        )
    gdf = gpd.GeoDataFrame(df, geometry=shapes.geometry.values, crs=shapes.crs)
    return gdf


# %% ../../notes/02_aggregate.ipynb 35
def aggregate_data(
        cfg: DictConfig,  # hydra configuration file
        input_file: str, # path to the input file
        output_file: str, # path to the output file
        exposure_variable: str # the variable to aggregate
    )->None:
    '''
    Run the agggregation step of the pipeline.

    Note, this function is the second step in the snakemake 
    pipeline. This means that in order to define the input
    file, we use the snakemake.input and snakemake.output variables
    injected into the runtime by snakemake.
    '''

    if cfg.development_mode:
        describe(cfg)
        return None
    
    # get the healthshed shapefile
    driver = GoogleDriver(json_key_path=here() / cfg.GOOGLE_DRIVE_AUTH_JSON.path)
    drive = driver.get_drive()
    healthsheds = driver.read_healthsheds(cfg.GOOGLE_DRIVE_AUTH_JSON.healthsheds_id)

    # get the aggregation configuration
    # exposure_variable = cfg.aggregation.variable
    agg_func = _get_callable(cfg.aggregation.daily.function)
    
    resampled_nc_file = resample_netcdf(input_file, agg_func=agg_func)

    resampled_tiff = netcdf_to_tiff(
        ds=resampled_nc_file,
        variable=exposure_variable,
        crs="EPSG:4326"
    )

    # run the polygon to raster cell function
    result_poly2cell=polygon_to_raster_cells(
        vectors = healthsheds.geometry.values, # the geometries of the shapefile of the regions
        raster=resampled_tiff.data, # the raster data above
        band=1, # the value of the day that we're using
        nodata=resampled_tiff.nodata, # any intersections with no data, may have to be np.nan
        affine=resampled_tiff.transform, # some math thing need to revise
        all_touched=True, 
        verbose=True
    )

    result = aggregate_to_healthsheds(
        res_poly2cell=result_poly2cell,
        raster=resampled_tiff,
        shapes=healthsheds,
        names_column="fs_uid",
        aggregation_func=agg_func,
        aggregation_name=exposure_variable
    )

    # Save the result to a file
    result.to_parquet(output_file)

# %% ../../notes/02_aggregate.ipynb 36
@hydra.main(version_base=None, config_path="../../conf", config_name="config")
def main(cfg: DictConfig) -> None:
    # Parse command-line arguments
    input_file = str(snakemake.input[0])  # First input file
    output_file = str(snakemake.output[0])
    aggregation_variable = str(snakemake.params.variable)

    aggregate_data(cfg, input_file=input_file, output_file=output_file, exposure_variable=aggregation_variable)

# %% ../../notes/02_aggregate.ipynb 37
#| eval: false
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False

if __name__ == "__main__" and not IN_NOTEBOOK:
    main()
